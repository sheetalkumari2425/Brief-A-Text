{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Learning Phase(2023-2024)\\\\Resume_projects\\\\Brief-A-Text\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Learning Phase(2023-2024)\\\\Resume_projects\\\\Brief-A-Text'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1 - Update config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 - Create entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 - Create configuration manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from briefAtext.constant import *\n",
    "from briefAtext.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.config = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            tokenizer_name = config.tokenizer_name\n",
    "        )\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 - Create components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-13 12:41:06,564: INFO: config: PyTorch version 2.4.1 available.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.briefAtext.logging import logger\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "    def convert_examples_to_features(self, example_batch):\n",
    "        input_encodings = self.tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "        return {\n",
    "            \"input_ids\": input_encodings['input_ids'],\n",
    "            \"attention_mask\": input_encodings['attention_mask'],\n",
    "            \"labels\": target_encodings['input_ids']\n",
    "        }\n",
    "    \n",
    "    def convert(self):\n",
    "        # dataset_samsum = load_dataset('csv', self.config.data_path)\n",
    "        dataset_samsum = load_from_disk('csv', self.config.data_path)\n",
    "        print(\"loaded\")\n",
    "        dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched=True)\n",
    "        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir, \"summarizer-data-latest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# class DataTransformation:\n",
    "#     def __init__(self, config: DataTransformationConfig):\n",
    "#         self.config = config\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "#     def convert_examples_to_features(self, example_batch):\n",
    "#         input_encodings = self.tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "#         with self.tokenizer.as_target_tokenizer():\n",
    "#             target_encodings = self.tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "#         return {\n",
    "#             \"input_ids\": input_encodings['input_ids'],\n",
    "#             \"attention_mask\": input_encodings['attention_mask'],\n",
    "#             \"labels\": target_encodings['input_ids']\n",
    "#         }\n",
    "\n",
    "#     def convert(self):\n",
    "#         # Ensure the path is valid\n",
    "#         valid_path = os.path.normpath(self.config.data_path)  # Normalize path for the OS\n",
    "\n",
    "#         # Ensure the path exists\n",
    "#         if os.path.exists(valid_path):\n",
    "#             print(f\"Loading dataset from {valid_path}\")\n",
    "#             dataset_samsum = load_dataset('csv', data_files=valid_path)\n",
    "#         else:\n",
    "#             raise FileNotFoundError(f\"Dataset path {valid_path} does not exist.\")\n",
    "        \n",
    "#         # Proceed with the conversion process\n",
    "#         dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched=True)\n",
    "#         dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir, \"summarizer-data-latest\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5- Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-13 12:41:06,885: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-09-13 12:41:06,895: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-09-13 12:41:06,898: INFO: common: Created directory at: artifacts]\n",
      "[2024-09-13 12:41:06,901: INFO: common: Created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "InvalidConfigName",
     "evalue": "Bad characters from black list '<>:/\\|?*' found in 'artifacts/data_ingestion/summarizer-data-latest'. They could create issues when creating a directory for this config on Windows filesystem.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidConfigName\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     data_transformation\u001b[38;5;241m.\u001b[39mconvert()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     data_transformation_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_data_transformation_config()\n\u001b[0;32m      4\u001b[0m     data_transformation \u001b[38;5;241m=\u001b[39m DataTransformation(config\u001b[38;5;241m=\u001b[39mdata_transformation_config)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mdata_transformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mDataTransformation.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     dataset_samsum \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# dataset_samsum = load_from_disk(self.config.data_path)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\datasets\\load.py:2606\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2602\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2603\u001b[0m )\n\u001b[0;32m   2605\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2606\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2607\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2608\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2609\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2610\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2611\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2612\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2613\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2614\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2615\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2616\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2617\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2618\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2619\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2620\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2621\u001b[0m )\n\u001b[0;32m   2623\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32md:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\datasets\\load.py:2314\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2312\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 2314\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m builder_cls(\n\u001b[0;32m   2315\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2316\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   2317\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m   2318\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2319\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2320\u001b[0m     \u001b[38;5;28mhash\u001b[39m\u001b[38;5;241m=\u001b[39mdataset_module\u001b[38;5;241m.\u001b[39mhash,\n\u001b[0;32m   2321\u001b[0m     info\u001b[38;5;241m=\u001b[39minfo,\n\u001b[0;32m   2322\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2323\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2324\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2325\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   2326\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2327\u001b[0m )\n\u001b[0;32m   2328\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32md:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\datasets\\builder.py:374\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_kwargs \u001b[38;5;241m=\u001b[39m config_kwargs\n\u001b[1;32m--> 374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_builder_config(\n\u001b[0;32m    375\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    376\u001b[0m     custom_features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[1;32md:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\datasets\\builder.py:614\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[1;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERSION:\n\u001b[0;32m    613\u001b[0m         config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERSION\n\u001b[1;32m--> 614\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIG_CLASS(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs)\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# otherwise use the config_kwargs to overwrite the attributes\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(builder_config) \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;28;01melse\u001b[39;00m builder_config\n",
      "File \u001b[1;32m<string>:49\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, name, version, data_dir, data_files, description, sep, delimiter, header, names, column_names, index_col, usecols, prefix, mangle_dupe_cols, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, skipfooter, doublequote, memory_map, float_precision, chunksize, features, encoding_errors, on_bad_lines, date_format)\u001b[0m\n",
      "File \u001b[1;32md:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\datasets\\packaged_modules\\csv\\csv.py:71\u001b[0m, in \u001b[0;36mCsvConfig.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter\n",
      "File \u001b[1;32md:\\Learning Phase(2023-2024)\\Resume_projects\\Brief-A-Text\\briefAtext\\lib\\site-packages\\datasets\\builder.py:132\u001b[0m, in \u001b[0;36mBuilderConfig.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m invalid_char \u001b[38;5;129;01min\u001b[39;00m INVALID_WINDOWS_CHARACTERS_IN_PATH:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m invalid_char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname:\n\u001b[1;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidConfigName(\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad characters from black list \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINVALID_WINDOWS_CHARACTERS_IN_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThey could create issues when creating a directory for this config on Windows filesystem.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files, (DataFilesDict, DataFilesPatternsDict)):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a DataFilesDict in data_files but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mInvalidConfigName\u001b[0m: Bad characters from black list '<>:/\\|?*' found in 'artifacts/data_ingestion/summarizer-data-latest'. They could create issues when creating a directory for this config on Windows filesystem."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
